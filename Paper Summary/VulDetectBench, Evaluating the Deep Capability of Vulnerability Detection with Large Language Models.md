Reviewed by: **Kenzy**

**Paper Name:** VulDetectBench: Evaluating the Deep Capability of Vulnerability Detection with Large Language Models

1. Programming Language:
    - C/C++
2. **Models / Techniques Used:**
    - **Models:**
        - **Closed source models:** GPT4, Gemini-Pro, ERNIE 4.0
        - **Open source models:** Mixtral (8×7B, 8×22B), Qwen (7B, 14B), Llama3 (8B, 70B), Llama2 (7B, 13B), CodeLlama (7B, 13B), Deepseek-coder (7B), ChatGLM3 (6B), Vicuna (7B, 13B)
    - **Benchmark Tasks:**
        1. **Vulnerability Existence Detection:** “Is this code vulnerable? (YES/NO)”
            - **Metrics:** **Accuracy** and **F1**.
        2. **CWE Type Inference:** “What CWE category is this bug?” (multiple choice with one optimal + one “ancestor” sub-optimal + 3 distractors).
            - **Metrics:**
                - **ME (Moderate Evaluation):** 1 point for the optimal choice and 1 point for choosing the suboptimal without the optimal
                - **SE (Strict Evaluation):** grants 1 point for the optimal and 0.5 points for the suboptimal, with no points for other selections.
        3. **Key Data Objects and Functions Identification:** Which variables / functions are central to this vulnerability?
            - _Metrics:_
                - Macro Recall (MAR) = (1/n) × Σ(i=1 to n) (TPᵢ / (TPᵢ + FNᵢ))
                - Micro Recall (MIR) = Σ(i=1 to n) TPᵢ / Σ(i=1 to n) (TPᵢ + FPᵢ)
        4. **Root Cause Location:** “Show the lines that are the bug’s root cause.”
            - _Metrics:_
                - **URS:** Union Recall Score = (1/n) × Σ(i=1 to n) (ILᵢ / ROLᵢ)
                - **ORS:** Overlap Recall Score = ORS = (1/n) × Σ(i=1 to n) (ILᵢ / ULᵢ)
                - ILᵢ = Intersection Lines for instance i
                - ROLᵢ = Result Output Lines for instance i
                - ULᵢ = Union Lines for instance i
        5. **Trigger Point Location:** “Which exact line(s) trigger the bug?”
            - _**Metrics:**_ **same as task 4**
    - **Setup:** NVIDIA GeForce RTX 4090 graphics cards. Single RTX 4090 units are used for models up to 7B, while two RTX 4090s are used in parallel for larger models (13B-14B) to maintain optimal conditions and avoid performance bottlenecks.
3. **Dataset Details:**
    - **Name:** VulDetectBench
    - **Availability:** [Here](https://github.com/Sweetaroo/VulDetectBench)
    - **Size:** Task 1: **1,000** entries; Task 2: **500**; Tasks 3–5: **100 each** (same 100 across T3–T5). Token length kept under ~4K per sample.
    - **Balance:** No
    - **Data Sources:**
        - SARD **Juliet C/C++**, **Big-Vul**, **Devign** (partially used)
        - **Real-Project Patch datasets** (OpenSSL, Wireshark, PostgreSQL, FFmpeg, GIMP, GNU Grep, Apache Subversion, Tree)
    - **Data Splitting across Tasks:** Tasks1 and 2 are designed to contain identical datasets, each comprising 1,000 data entries. Similarly, Tasks3, 4, and 5 are standardized with exactly the same sample data, encompassing 100 data entries foreach task.

|No.|Task|Number of Entries|CWE Types|TOP 5 CWE Type|Min - MAX tokens|
|---|---|---|---|---|---|
|1|Vulnerability Existence Detection|1000|48|78 \ 90 \ 23 \ 36 \ 114|50 - 493|
|2|Vulnerability CWE Type Inference|500|48|Same Above|265 - 3372|
|3|Key Objects and Functions Identification|100|38|476 \ 191 \ 89 \ 78 \ 89|1017 - 3690|
|4|Vulnerability Root Cause Location|100|38|Same Above|1010 - 3262|
|5|Vulnerability Trigger Point Location|100|38|Same Above|1011 - 3363|

1. **Performance Metrics Reported:**

|Model|Size|Task1 (ACC \ F1)|Task2 (SE \ ME)|Task 3 (MAR \ MIR)|Task 4 (URS \ ORS)|Task 5 (URS \ ORS)|
|---|---|---|---|---|---|---|
|GPT4|-|71.43 \ 79.30|**92.96 \ 95.17**|20.21 \ 16.07|**24.26 \ 27.07**|13.00 \ 17.85|
|ERNIE4.0|-|**85.01 \ 86.65**|72.50 \ 70.00|27.87 \ 22.54|11.77 \ 27.99|22.43 \ 10.38|
|Gemini-pro|-|73.10 \ 75.74|70.10 \ 77.00|13.03 \ 10.55|14.64 \ 23.51|07.56 \ 18.89|
|Deepseek|7B|71.30 \ 61.16|37.60 \ 40.20|17.81 \ 13.67|05.36 \ 09.22|04.30 \ 08.83|
|Qwen|7B|62.30 \ 43.31|60.10 \ 62.20|13.63 \ 11.15|04.34 \ 10.09|05.56 \ 10.95|
|Qwen|14B|69.10 \ 55.67|77.50 \ 82.80|16.49 \ 13.67|06.40 \ 08.32|03.14 \ 04.81|
|ChatGLM3|6B|69.90 \ 65.37|39.90 \ 46.60|00.16 \ 00.12|00.19 \ 01.33|00.42 \ 01.12|
|Vicuna|13B|57.60 \ 31.17|67.30 \ 74.40|07.30 \ 06.35|0 \ 0|0 \ 0|
|Vicuna|7B|48.60 \ 65.27|31.30 \ 42.00|07.79 \ 06.24|0 \ 0|0 \ 0|
|CodeLlama|13B|47.90 \ 58.81|32.80 \ 44.60|10.34 \ 08.51|03.30 \ 03.47|01.29 \ 01.89|
|CodeLlama|7B|36.40 \ 53.37|36.70 \ 41.60|06.28 \ 05.04|01.69 \ 02.55|00.69 \ 01.31|
|Llama3|8B|69.40 \ 76.53|62.30 \ 68.40|22.83 \ 17.99|00.19 \ 01.10|00.17 \ 00.53|
|Llama2|7B|47.90 \ 64.19|41.40 \ 54.40|11.47 \ 09.59|0 \ 0|0 \ 0|
|Llama2|13B|70.37 \ 73.67|58.70 \ 67.40|10.99 \ 09.23|00.50 \ 01.20|00.10 \ 00.20|
|Llama3|70B|47.45 \ 60.33|26.00 \ 17.00|09.18 \ 07.43|0 \ 0|0 \ 0|
|Mixtral|8*7B|76.42 \ 79.00|62.00 \ 58.40|21.61 \ 17.51|01.66 \ 04.28|05.74 \ 03.51|
|Mixtral|8*22B|81.82 \ 84.47|77.80 \ 74.80|**30.26 \ 24.10**|17.49 \ 11.46|06.17 \ 02.83|

1. **Strengths:**
    - The paper offered a good Benchmark with 5 comprehensive tasks
2. **Weaknesses:**
    - Data in benchmark needs more cleansing due to the probable data leakage and duplication
    - Models still need more and more improvement as it performs awfully in the higher complexity tasks (3:5)