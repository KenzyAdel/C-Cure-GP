Reviewed by: **Kenzy**

**Paper Name:** Vulnerability Detection with Code Language. Models: How Far Are We?

1. **Programming Language(s):**
    - C/C++
2. **Models / Techniques Used:**
    - **Data gathering techniques:**
        - Merging all security-related commits and functions changed by them from BigVul , CrossVul, CVEfixes , and DiverseVul.
        - Exclude data from Devign/CodeXGLUE because large portion of its commits is unrelated to security issues
        - Remove of duplicate code to overcome data leakage
        - **Chronological splitting:** split the data based on the commit data, so that the oldest 80% will be the train set, 10% in the middle will be the validation set, and the most recent 10% will be the test set
    - **Labeling techniques:**
        - **PRIMEVUL-ONEFUNC**: Only labels functions as vulnerable if they're the only function changed in a security fix
        - **PRIMEVUL-NVDCHECK**: Uses expert analysis from CVE entries to verify vulnerabilities
    - **New Evaluation techniques:**
        - **Vulnerability Detection Score (VD-S):** Measures how many real vulnerabilities are missed (false negative rate) when the false positive rate is kept below 0.5%
        - **Pair-wise Evaluation:** assess the model’s ability to distinguish between a vulnerable code sample and its benign (fixed) counterpart
    - **Models:**
        - Open source models: CODET5, CODEBERT, UNIXCODER, STARCODER2, CODEGEN2.5
        - larger language models(LLMs): GPT-3.5, GPT-4
3. **Dataset Details:**
    - **Name:** PRIMEVUL
    - **Availability:** [Here](https://github.com/DLVulDet/PrimeVul)
    - **Size:** 6,968 vulnerable and 228,800 benign functions across 755 projects and 6,827 commits.
    - **Balance:** No
4. **Performance Metrics Reported:**
    - **Performance of code-LM-based vulnerability detection models:**

|Model|Train|Test|Acc↑|F1|VD-S↓|P-C↑|P-V↓|P-B↓|P-R↓|
|---|---|---|---|---|---|---|---|---|---|
|CodeT5|BigVul|BigVul|95.67|64.93|77.30|24.98|50.90|22.79|1.33|
||BigVul|PRIMEVUL|97.00|5.82|95.97|0.18|3.01|96.10|0.71|
||PRIMEVUL|PRIMEVUL|96.67|19.7|89.93|1.06|12.94|84.75|1.24|
|CodeBERT|BigVul|BigVul|95.57|62.88|81.77|22.60|48.34|27.83|1.23|
||BigVul|PRIMEVUL|97.04|4.49|95.54|0.35|1.95|96.99|0.71|
||PRIMEVUL|PRIMEVUL|96.87|20.86|88.78|1.77|11.35|86.17|0.71|
|UnixCoder|BVBigVul|BigVul|96.46|65.46|62.30|39.60|23.74|33.24|3.42|
||BigVul|PRIMEVUL|97.27|1.94|95.11|0.35|0.35|98.76|0.53|
||PRIMEVUL|PRIMEVUL|96.86|21.43|89.21|1.60|12.06|85.11|1.24|
|StarCoder2|BigVul|BigVul|96.20|68.26|69.14|35.23|41.98|20.61|2.18|
||BigVul|PRIMEVUL|97.09|3.09|96.83|0.89|0.89|97.70|0.53|
||PRIMEVUL|PRIMEVUL|97.02|18.05|89.64|2.30|8.16|88.30|1.24|
|CodeGen2.5|BigVul|BigVul|96.57|67.30|61.73|40.84|26.02|29.63|3.51|
||BigVul|PRIMEVUL|97.23|1.91|95.68|1.24|0.00|98.76|0.00|
||PRIMEVUL|PRIMEVUL|96.65|19.61|91.51|3.01|10.82|84.22|1.95|

- **Performance of LLMs on PRIMEVUL paired functions:**

|Model|Method|P-C↑|P-V↓|P-B↓|P-R↓|
|---|---|---|---|---|---|
|GPT-3.5|Two-shot|5.67|13.83|77.84|2.66|
||Chain-of-Thought prompting|6.21|4.79|83.51|5.50|
||Fine-tune|1.24|5.32|90.96|2.48|
|GPT-4|Two-shot|5.14|71.63|21.45|1.77|
||Chain-of-Thought prompting|12.94|54.26|24.47|8.33|
|RANDOM GUESS|-|22.70|26.24|26.42|24.65|

**Note:**

- **Pair-wise Correct Prediction (P-C):** The model correctly predicts the ground-truth labels for both elements of a pair.
- **Pair-wise Vulnerable Prediction (P-V):** The model incorrectly predicts both elements of the pair as vulnerable.
- **Pair-wise Benign Prediction (P-B):** The model incorrectly predicts both elements of the pair as benign.
- **Pair-wise Reversed Prediction (P-R):** The model incorrectly and inversely predicts the labels for the pair.
- Accuracy is not an appropriate metric for vulnerability detection, most samples in realistic benchmarks are not vulnerable, it is possible to achieve high accuracy by always predicting “not vulnerable”.
- A high accuracy score does not necessarily signify effective detection of security vulnerabilities; it may simply reflect the accurate identification of non-vulnerable cases, or a bias towards predicting “not vulnerable”
- F1 score reflects both false positives and false negatives by combining them into a single penalty. Yet, for VD tools in practice, the overwhelming majority of code is not vulnerable, so a critical challenge is preventing excessive false alarms.
- The F1 score fails to reflect this asymmetry, so tools with a high F1 score may be useless in practice.

1. **Strengths:**
    - systematic evaluation of datasets and exposing quality issues
    - PRIMEVUL with 86-92% label accuracy vs. 25-60% in existing datasets
    - Introduced VD-S and pair-wise evaluation for practical assessment
2. **Weaknesses:**
    - **Models need for More Context:** To enable such a process, PRIMEVUL maintains the metadata for the included commits, providing resources to extract relevant contexts.
    - **Models make decisions primarily based on textual similarity:** To enable such a process, teach code LMs about security concepts, such as pre-training methods. Also you can build **hybrid systems** that combine LMs with traditional security analysis tools.
    - **posing vulnerability detection as a binary classification problem and teaching the Code LMs accordingly might be too simplistic.**
    - **There is still a space of improvement in the labeling accuracy:** PrimeVul dataset isn’t 100% accurate like **SVEN** dataset that is manually labeled.